{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3496528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"Read File\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7f4d6-4620-489d-acc2-edb4f2fb7e76",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7a2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/spark 03 - filter quizz.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfcbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vals(x):\n",
    "    return [len(i) for i in x.split(' ')]\n",
    "\n",
    "rdd2 = rdd.map(count_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d410367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 7, 6], [3, 3, 3, 3, 6], [5, 6, 6, 2, 7, 5], [6, 3, 5, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73dfff",
   "metadata": {},
   "source": [
    "## FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423f960d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 7, 6, 3, 3, 3, 3, 6, 5, 6, 6, 2, 7, 5, 6, 3, 5, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3 = rdd.flatMap(count_vals)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1b2634-03bb-4ff1-864a-21626094244c",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1b37b8-2f3c-4ced-8e2c-e67dba67891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_filter = rdd.filter(lambda x: x )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100f65a-46c0-439f-baed-892ebe2c085c",
   "metadata": {},
   "source": [
    "## Quizz Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbdaf22b-72d6-4b88-90e7-49e3f8f61a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/spark 03 - filter quizz.txt\")\n",
    "rdd = rdd.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723322a2-b10d-4904-8b18-84a36c5d1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(text):\n",
    "    for word in text:\n",
    "        if word.startswith(\"a\") or word.startswith(\"c\"):\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880ece8a-e33e-405c-a95e-1edb52d8f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_filtered = rdd.filter(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f0cab4-17e6-4125-9f0b-af8b849ebcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'mango', 'dog', 'mic', 'laptop', 'switch', 'mobile']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filtered.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98437d2e-5378-440d-ac59-b049a774fdc3",
   "metadata": {},
   "source": [
    "## RDD Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "985cdbc4-5f8e-487e-894a-d7f042513cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/sample_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dfe5f6c-984e-4cfb-bd94-309e6796be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5edefd06-e6f7-40bf-844f-fa272288b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb276e0-ec6c-45f6-bc99-4a42f7f952d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '98',\n",
       " '',\n",
       " '8',\n",
       " '12',\n",
       " '9',\n",
       " '86',\n",
       " '786',\n",
       " '56',\n",
       " '66',\n",
       " '872',\n",
       " '27',\n",
       " '11',\n",
       " '5',\n",
       " '6',\n",
       " '567',\n",
       " '87',\n",
       " '678',\n",
       " '5675']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f390c-4347-4562-86b0-f9a4d1bccf04",
   "metadata": {},
   "source": [
    "## RDD Functions - groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db992a01-e364-4ecf-bd40-4a2d8bba474a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ['this']),\n",
       " (6, ['animal', 'laptop', 'switch', 'mobile', 'amanda']),\n",
       " (2, ['am']),\n",
       " (5, ['mango', 'chair', 'cover', 'alarm']),\n",
       " (7, ['company', 'charger']),\n",
       " (3, ['cat', 'dog', 'ant', 'mic', 'any', 'ant'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"data/spark 03 - filter quizz.txt\")\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(' '))\n",
    "rdd3 = rdd2.map(lambda x: (len(x), x))\n",
    "rdd3.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e36f69-ab06-4069-a99e-16cd308f5d12",
   "metadata": {},
   "source": [
    "## RDD Functions - reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ff50bd-e262-4d51-ad1c-9f23a4e8e388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'this'),\n",
       " (6, 'animal laptop switch mobile amanda'),\n",
       " (2, 'am'),\n",
       " (5, 'mango chair cover alarm'),\n",
       " (7, 'company charger'),\n",
       " (3, 'cat dog ant mic any ant')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"data/spark 03 - filter quizz.txt\")\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(' '))\n",
    "rdd3 = rdd2.map(lambda x: (len(x), x))\n",
    "\n",
    "rdd3.reduceByKey(lambda x, y: x + \" \" + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1059fdb8-2a4f-49ec-9489-e83afadb2296",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o163.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/D:/work/dev/python-pyspark/sample_file.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12544/1268113676.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrdd2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrdd2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\bin\\programs\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[1;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m         \"\"\"\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\bin\\programs\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[1;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[0;32m   2134\u001b[0m         \"\"\"\n\u001b[0;32m   2135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2136\u001b[1;33m             \u001b[0mnumPartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[0mserializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\bin\\programs\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2579\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2580\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2581\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\bin\\programs\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2935\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\bin\\programs\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\bin\\programs\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\bin\\programs\\spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/D:/work/dev/python-pyspark/sample_file.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"sample_file.txt\")\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1))\n",
    "\n",
    "rdd2.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353465a-bbb5-4ebd-84f4-e5d3050ea943",
   "metadata": {},
   "source": [
    "## Quizz - Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac534c6-db36-44fc-9d37-83eebcbd1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"spark 03 - filter quizz.txt\")\n",
    "rdd = rdd.flatMap(lambda x: x.split(' '))\n",
    "rdd.map(lambda x: (x, 1)).filter(lambda x: len(x) != 0).reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e18faf-3595-4bf9-b770-b064b216bbf5",
   "metadata": {},
   "source": [
    "## Actions - count()\n",
    "Cuenta los elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133d00a-464a-4ac1-95f5-9e49f27d1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"spark 03 - filter quizz.txt\")\n",
    "rdd.flatMap(lambda x: x.split(' ')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c748f-b5b1-4b13-b751-a9772c57e025",
   "metadata": {},
   "source": [
    "## Actions - countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31e403-4bef-468a-b1e1-2abf7f0463ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.flatMap(lambda x: x.split(' ')).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cccee-18d3-405c-8782-1a5e879cb8f9",
   "metadata": {},
   "source": [
    "## saveAsTextFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb0350-7bb1-4d46-9cb6-ef9f3457b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/spark 03 - filter quizz.txt\")\n",
    "print(rdd.getNumPartitions())\n",
    "rdd.flatMap(lambda x: x.split(' ')).saveAsTextFile(\"output/saveAsText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5378d-08ef-4a88-b989-fe24274f61a9",
   "metadata": {},
   "source": [
    "## coalesce() and repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee2ec3da-deb9-4825-8878-5570dbe46e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"data/spark 03 - filter quizz.txt\")\n",
    "rdd = rdd.repartition(5)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fbd0a-b06f-4efc-8479-d73d0e048ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
